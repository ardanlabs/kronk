# ==============================================================================
# CONFIG OPTIONS
#
# example-model:
#   context-window: 8192                 # Max tokens model can process (default: 8192)
#   nbatch: 2048                         # Logical batch size (default: 2048)
#   nubatch: 512                         # Physical batch size for prompt ingestion (default: 512)
#   nthreads: 0                          # Threads for generation (0 = llama.cpp default)
#   nthreads-batch: 0                    # Threads for batch processing (0 = llama.cpp default)
#   cache-type-k: q8_0                   # KV cache key type: f32, f16, q8_0, q4_0, bf16, auto
#   cache-type-v: q8_0                   # KV cache value type: f32, f16, q8_0, q4_0, bf16, auto
#   flash-attention: enabled             # Flash Attention: enabled, disabled, auto
#   device: ""                           # Device to use (run llama-bench --list-devices)
#   nseq-max: 0                          # Max parallel sequences for batched inference (0 = default)
#   offload-kqv: true                    # Offload KV cache to GPU (false = keep on CPU)
#   op-offload: true                     # Offload tensor operations to GPU (false = keep on CPU)
#   ngpu-layers: 0                       # GPU layers to offload (0 = all, -1 = none, N = specific count)
#   split-mode: row                      # Multi-GPU split: none, layer, row (row recommended for MoE models)
#   system-prompt-cache: false           # Cache system prompt KV state for reuse across requests
#   first-message-cache: false           # Cache first user message KV state (for clients like Cline)
#   cache-min-tokens: 100                # Min tokens before caching (default: 100, applies to both cache types)

# ==============================================================================
# MODEL SELECTION NOTES
#
# These suffixes define how a Large Language Model (LLM) is quantizedâ€”compressed
# from its original size to fit into computer memory (RAM/VRAM). The primary
# difference lies in the balance between model size, inference speed, and output
# accuracy (intelligence).
#
# - F16 (FP16): Unquantized, maximum quality, largest size.
# - Q8_0: 8-bit, near-lossless compression, very high quality. (Best for local)
# - K_XL: K-quant (smart) method, high efficiency, better accuracy than
#         standard _0 at similar sizes.
#
# Also, the FP8/F16 or even FP4 are more for modern nvidia cards,are all the
# precision of the floating point numbers. All operations are accelerated by
# the GPU.
#
# Most of the modern GPUs have the hardware for FP16, and other formats, like FP8
# (Q8) or FP4 are emulated.
#
# If you have hardware support for FP8, it will be roughly double the speed of FP16.
#
# There are also INT4/INT8 models, but that's already weird. They take the
# floating point numbers and convert them to the int in the given precision.
#
# For us, on Apple M4 chips, there's still some performance to be gained.
# See: https://github.com/ggml-org/llama.cpp/discussions/336
# However, llama.cpp doesn't currently support the MLX framework.

# ==============================================================================
# CONFIGURATION NOTES
#
# If you want to try using a model with an Agent, use these settings:
#   nseq-max: 1
#   first-message-cache: true
#
# If you want to use a Chat application like OpenWebUI use these settings:
#   nseq-max: 1
#   system-prompt-cache: true
#
# Cline is working great with Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL and
# cerebras_qwen3-coder-reap-25b-a3b-q8_0.
#
# Claude Code needs a model that handles tool calling well with a decent
# context window. I have not found a model yet that works well with Claude Code.
#
# Stay away from using the gpt-oss model of any size with agents. It will not
# behave in several aspects, but specfically it will not listen.

# ==============================================================================
# TEXT INFERENCE MODELS

# ------------------------------------------------------------------------------
# Coding Agents and configured for Cline or Claude Code

# This has been the best with Cline and OpenCode.
Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL:
  &base_Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2
  sampling-parameters:
    temperature: 0.7
    top_p: 0.8
    top_k: 20

Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL/FMC:
  <<: *base_Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL
  nseq-max: 1
  first-message-cache: true

Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL/SPC:
  <<: *base_Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL
  nseq-max: 1
  system-prompt-cache: true

# Smaller version of the 30B model and works well.
cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0:
  &base_cerebras_qwen3-coder-reap-25b-a3b-q8_0
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0/FMC:
  <<: *base_cerebras_qwen3-coder-reap-25b-a3b-q8_0
  nseq-max: 1
  first-message-cache: true

cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0/SPC:
  <<: *base_cerebras_qwen3-coder-reap-25b-a3b-q8_0
  nseq-max: 1
  system-prompt-cache: true

# This has worked with Cline and OpenCode but just slow.
GLM-4.7-Flash-UD-Q8_K_XL: &base_GLM_4_7_Flash_UD_Q8_K_XL
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2
  sampling-parameters:
    temperature: 1.0
    min_p: 0.01
    top_p: 0.95

GLM-4.7-Flash-UD-Q8_K_XL/FMC:
  <<: *base_GLM_4_7_Flash_UD_Q8_K_XL
  nseq-max: 1
  first-message-cache: true
  sampling-parameters:
    temperature: 0.7
    top_p: 1.0

GLM-4.7-Flash-UD-Q8_K_XL/SPC:
  <<: *base_GLM_4_7_Flash_UD_Q8_K_XL
  nseq-max: 1
  system-prompt-cache: true

# ------------------------------------------------------------------------------
# Good Reasoning models, tooling support, don't use with Agents.

Qwen3-Coder-30B-A3B-Instruct-Q8_0:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

gpt-oss-20b-Q8_0:
  context-window: 98304
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

# Don't use with Agents.
gpt-oss-120b-F16: &base_gpt-oss-120b-F16
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

gpt-oss-120b-F16/SPC:
  <<: *base_gpt-oss-120b-F16
  nseq-max: 1
  system-prompt-cache: true

# Great model but small context-window :(
Qwen3-8B-Q8_0:
  context-window: 40960
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

# ------------------------------------------------------------------------------
# Non-Reasoning models, tooling support, don't use with Agents.

# Model runs slow and haven't tested it too much.
GLM-4.7-Flash-Q8_0:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2
  sampling-parameters:
    temperature: 1.0
    min_p: 0.01
    top_p: 0.95

# ==============================================================================
# VISION MODELS

# Vision model that is working great.
Qwen2.5-VL-3B-Instruct-Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 2048
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled

# ==============================================================================
# AUDIO MODELS

# Audio model that is working great.
Qwen2-Audio-7B.Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled

# ==============================================================================
# EMBEDDING MODELS

# Embedding model that is working great.
embeddinggemma-300m-qat-Q8_0:
  context-window: 2048
  nbatch: 1024
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
