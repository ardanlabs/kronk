# example-model:
#   context-window: 8192      # Max tokens model can process (default: 8192)
#   nbatch: 2048              # Logical batch size (default: 2048)
#   nubatch: 512              # Physical batch size for prompt ingestion (default: 512)
#   nthreads: 0               # Threads for generation (0 = llama.cpp default)
#   nthreads-batch: 0         # Threads for batch processing (0 = llama.cpp default)
#   cache-type-k: q8_0        # KV cache key type: f32, f16, q8_0, q4_0, bf16, auto
#   cache-type-v: q8_0        # KV cache value type: f32, f16, q8_0, q4_0, bf16, auto
#   flash-attention: enabled  # Flash Attention: enabled, disabled, auto
#   device: ""                # Device to use (run llama-bench --list-devices)

gpt-oss-20b-Q8_0:
  context-window: 32768
Qwen3-8B-Q8_0:
  context-window: 32768
  cache-type-k: f16
  cache-type-v: f16
  nbatch: 1024
  nubatch: 256
