# ==============================================================================
# CONFIG OPTIONS
#
# example-model:
#   context-window: 8192                 # Max tokens model can process (default: 8192)
#   nbatch: 2048                         # Logical batch size (default: 2048)
#   nubatch: 512                         # Physical batch size for prompt ingestion (default: 512)
#   nthreads: 0                          # Threads for generation (0 = llama.cpp default)
#   nthreads-batch: 0                    # Threads for batch processing (0 = llama.cpp default)
#   cache-type-k: q8_0                   # KV cache key type: f32, f16, q8_0, q4_0, bf16, auto
#   cache-type-v: q8_0                   # KV cache value type: f32, f16, q8_0, q4_0, bf16, auto
#   flash-attention: enabled             # Flash Attention: enabled, disabled, auto
#   device: ""                           # Device to use (run llama-bench --list-devices)
#   nseq-max: 0                          # Max parallel sequences for batched inference (0 = default)
#   offload-kqv: true                    # Offload KV cache to GPU (false = keep on CPU)
#   op-offload: true                     # Offload tensor operations to GPU (false = keep on CPU)
#   ngpu-layers: 0                       # GPU layers to offload (0 = all, -1 = none, N = specific count)
#   split-mode: row                      # Multi-GPU split: none, layer, row (row recommended for MoE models)
#   system-prompt-cache: false           # Cache system prompt KV state for reuse across requests
#   incremental-cache: false             # Incremental message caching for agentic workflows (Cline, OpenCode)
#   cache-min-tokens: 100                # Min tokens before caching (default: 100, applies to both cache types)
#   rope-scaling-type: yarn              # RoPE scaling: none, linear, yarn
#   yarn-orig-ctx: 32768                 # Original training context size (nil = from model)
#   rope-freq-base: 1000000              # RoPE base frequency (nil = from model, e.g., 10000 Llama, 1000000 Qwen)
#   rope-freq-scale: 0.25                # RoPE frequency scale (nil = auto-calculated)
#   yarn-ext-factor: 1.0                 # YaRN extrapolation mix factor (nil = auto, 0 = disable)
#   yarn-attn-factor: 1.0                # YaRN attention magnitude scaling (nil = default 1.0)
#   yarn-beta-fast: 32.0                 # YaRN low correction dimension (nil = default 32.0)
#   yarn-beta-slow: 1.0                  # YaRN high correction dimension (nil = default 1.0)

# ==============================================================================
# ROPE AND YARN NOTES
#
# YaRN (Yet another RoPE extensioN) enables extended context windows beyond a
# model's native training length. For example, Qwen3 models trained on 32k can
# support 131k context with YaRN scaling.
#
# Minimal configuration for YaRN (llama.cpp auto-calculates the rest):
#   rope-scaling-type: yarn
#   yarn-orig-ctx: 32768       # Original training context size
#   context-window: 131072     # Desired extended context
#
# Auto-calculated defaults when using YaRN:
#   rope-freq-base    - Read from model metadata (Qwen3 = 1000000, Llama = 10000)
#   rope-freq-scale   - Auto-calculated from context ratio
#   yarn-ext-factor   - Auto-calculated (nil triggers this)
#   yarn-attn-factor  - 1.0
#   yarn-beta-fast    - 32.0
#   yarn-beta-slow    - 1.0
#
# The advanced parameters exist for models with non-standard RoPE configurations
# (e.g., DeepSeek-V2). For typical YaRN usage, the minimal config is sufficient.
#
# Note: Extended context increases KV cache memory proportionally. A 4x context
# extension (32k → 131k) requires 4x the KV cache VRAM per slot.

# ==============================================================================
# CONFIGURATION NOTES
#
# SPC and IMC are mutually exclusive - choose one based on your use case
# and model behavior.
#
# SYSTEM PROMPT CACHE (SPC)
# -------------------------
# Caches only the system prompt's KV state in a RAM buffer. On each request
# the cached state is restored instantly, skipping system prompt re-decoding.
# The rest of the conversation (all user/assistant messages) is still
# prefilled every time.
#
# Choose SPC when:
#   - OpenWebUI and similar chat interfaces
#   - Applications with a consistent system prompt
#   - Multi-user scenarios with different system prompts
#
#   system-prompt-cache: true
#
# INCREMENTAL MESSAGE CACHE (IMC)
# -------------------------------
# Caches the entire conversation (all messages except the last) in the slot's
# KV cache and extends it incrementally each turn. Only the newest message
# needs prefilling, making long multi-turn conversations dramatically faster.
#
# Choose IMC when:
#   - AI coding agents
#   - Long-running agent conversations
#   - Any workflow where messages are appended, not edited
#   - Sub-agent architectures with multiple concurrent agents
#
#   incremental-cache: true
#
# MISCELLANEOUS NOTES
# -------------------------------
# You will notice with the Qwen3-8B-Q8_0 model a YARN configuration. This is
# how you can extend the models context window when you need it.

# ==============================================================================
# TEXT INFERENCE MODELS

Qwen3-Coder-Next-Q8_0/CLINE:
  nseq-max: 1
  incremental-cache: true

Qwen3-Coder-Next-Q8_0/KILO:
  nseq-max: 3
  incremental-cache: true

Qwen3-Coder-Next-Q8_0/SPC:
  nseq-max: 1
  system-prompt-cache: true

# ------------------------------------------------------------------------------

Qwen3.5-35B-A3B-UD-Q8_K_XL/CLINE:
  nseq-max: 1
  incremental-cache: true
  sampling-parameters:
    temperature: 0.6
    top_k: 20
    top_p: 0.95
    dry_multiplier: 2.0
    presence_penalty: 1.5

Qwen3.5-35B-A3B-UD-Q8_K_XL/KILO:
  nseq-max: 3
  incremental-cache: true
  sampling-parameters:
    temperature: 0.6
    top_k: 20
    top_p: 0.95
    dry_multiplier: 2.0
    presence_penalty: 1.5
    grammar: hermestoolcalling.grm

Qwen3.5-35B-A3B-UD-Q8_K_XL/SPC:
  nseq-max: 1
  system-prompt-cache: true

# ------------------------------------------------------------------------------

Qwen_Qwen3.5-35B-A3B-Q8_0/CLINE:
  nseq-max: 1
  incremental-cache: true
  sampling-parameters:
    temperature: 0.6
    top_k: 20
    top_p: 0.95
    dry_multiplier: 2.0
    presence_penalty: 1.5

Qwen_Qwen3.5-35B-A3B-Q8_0/KILO:
  nseq-max: 3
  incremental-cache: true
  sampling-parameters:
    temperature: 0.6
    top_k: 20
    top_p: 0.95
    dry_multiplier: 2.0
    presence_penalty: 1.5
    grammar: hermestoolcalling.grm

Qwen_Qwen3.5-35B-A3B-Q8_0/SPC:
  nseq-max: 1
  system-prompt-cache: true

# ------------------------------------------------------------------------------

gpt-oss-120b-F16/CLINE:
  nseq-max: 1
  incremental-cache: true

gpt-oss-120b-F16/KILO:
  nseq-max: 3
  incremental-cache: true

gpt-oss-120b-F16/SPC:
  nseq-max: 1
  system-prompt-cache: true

# ------------------------------------------------------------------------------

cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0/IMC:
  nseq-max: 1
  incremental-cache: true

# ------------------------------------------------------------------------------

# Extended context with YaRN scaling (32k training → 131k).
Qwen3-8B-Q8_0/YARN:
  context-window: 131072
  rope-scaling-type: yarn
  yarn-orig-ctx: 32768

# System Prompt Cache variant for testing.
# DO NOT REMOVE
Qwen3-8B-Q8_0/SPC:
  nseq-max: 1
  system-prompt-cache: true

# Incremental Message Cache variant for testing.
# DO NOT REMOVE
Qwen3-8B-Q8_0/IMC:
  nseq-max: 1
  incremental-cache: true

Qwen3-8B-Q8_0/CLINE:
  context-window: 131072
  rope-scaling-type: yarn
  yarn-orig-ctx: 32768
  nseq-max: 1
  incremental-cache: true
  sampling-parameters:
    enable_thinking: false
