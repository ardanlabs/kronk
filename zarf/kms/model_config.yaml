# ==============================================================================
# CONFIG OPTIONS
#
# example-model:
#   context-window: 8192                 # Max tokens model can process (default: 8192)
#   nbatch: 2048                         # Logical batch size (default: 2048)
#   nubatch: 512                         # Physical batch size for prompt ingestion (default: 512)
#   nthreads: 0                          # Threads for generation (0 = llama.cpp default)
#   nthreads-batch: 0                    # Threads for batch processing (0 = llama.cpp default)
#   cache-type-k: q8_0                   # KV cache key type: f32, f16, q8_0, q4_0, bf16, auto
#   cache-type-v: q8_0                   # KV cache value type: f32, f16, q8_0, q4_0, bf16, auto
#   flash-attention: enabled             # Flash Attention: enabled, disabled, auto
#   device: ""                           # Device to use (run llama-bench --list-devices)
#   nseq-max: 0                          # Max parallel sequences for batched inference (0 = default)
#   offload-kqv: true                    # Offload KV cache to GPU (false = keep on CPU)
#   op-offload: true                     # Offload tensor operations to GPU (false = keep on CPU)
#   ngpu-layers: 0                       # GPU layers to offload (0 = all, -1 = none, N = specific count)
#   split-mode: row                      # Multi-GPU split: none, layer, row (row recommended for MoE models)
#   system-prompt-cache: false           # Cache system prompt KV state for reuse across requests
#   first-message-cache: false           # Cache first user message KV state (for clients like Cline)
#   cache-min-tokens: 100                # Min tokens before caching (default: 100, applies to both cache types)
#   default-context:                     # Default sampling parameters (request values take precedence)
#     temperature: 0.8                   # Randomness of output (default: 0.8)
#     top_k: 40                          # Limit to top K probable tokens (default: 40)
#     top_p: 0.9                         # Nucleus sampling threshold (default: 0.9)
#     min_p: 0.0                         # Dynamic sampling threshold (default: 0.0)
#     max_tokens: 0                      # Max tokens to generate (0 = no limit)
#     repeat_penalty: 1.1                # Penalty for repeated tokens (default: 1.1)
#     repeat_last_n: 64                  # Tokens to consider for repeat penalty (default: 64)
#     dry_multiplier: 0.0                # DRY sampler multiplier (0 = disabled)
#     dry_base: 1.75                     # DRY exponential base (default: 1.75)
#     dry_allowed_length: 2              # Min n-gram length for DRY (default: 2)
#     dry_penalty_last_n: 0              # DRY context limit (0 = full context)
#     xtc_probability: 0.0               # XTC culling probability (0 = disabled)
#     xtc_threshold: 0.1                 # XTC probability threshold (default: 0.1)
#     xtc_min_keep: 1                    # Min tokens to keep after XTC (default: 1)
#     enable_thinking: "true"            # Enable thinking mode (default: "true")
#     reasoning_effort: "medium"         # GPT reasoning: none, minimal, low, medium, high
#     return_prompt: false               # Include prompt in response (default: false)
#     include_usage: true                # Include token usage in streaming (default: true)
#     logprobs: false                    # Return log probabilities (default: false)
#     top_logprobs: 0                    # Number of top logprobs to return (0-5)
#     stream: false                      # Stream response as SSE (default: false)

# ==============================================================================
# MODEL SELECTION NOTES
#
# These suffixes define how a Large Language Model (LLM) is quantized—compressed
# from its original size to fit into computer memory (RAM/VRAM). The primary
# difference lies in the balance between model size, inference speed, and output
# accuracy (intelligence).
#
# - F16 (FP16): Unquantized, maximum quality, largest size.
# - Q8_0: 8-bit, near-lossless compression, very high quality. (Best for local)
# - K_XL: K-quant (smart) method, high efficiency, better accuracy than
#         standard _0 at similar sizes.
#
# Also, the FP8/F16 or even FP4 are more for modern nvidia cards,are all the
# precision of the floating point numbers. All operations are accelerated by
# the GPU.
#
# Most of the modern GPUs have the hardware for FP16, and other formats, like FP8
# (Q8) or FP4 are emulated.
#
# If you have hardware support for FP8, it will be roughly double the speed of FP16.
#
# There are also INT4/INT8 models, but that's already weird. They take the
# floating point numbers and convert them to the int in the given precision.
#
# For us, on Apple M4 chips, there's still some performance to be gained.
# See: https://github.com/ggml-org/llama.cpp/discussions/336
# However, llama.cpp doesn't currently support the MLX framework.

# ==============================================================================
# SLOT MEMORY AND TOTAL VRAM COST FORMULA
#
# These figures are for KV cache VRAM only (when offload-kqv: true).
# Model weights require additional VRAM: ~7GB (7B Q8) or ~70GB (70B Q8).
# Total VRAM = model weights + KV cache.
#
# Memory is statically allocated upfront when the model loads,
# based on n_ctx × n_seq_max. Reserving slots consumes memory whether or not
# they're actually used.
#
# Example Calculations:
#
# This is how you calculate the amount of KV memory you need per slot.
#
# KV_Per_Token_Per_Layer = head_count_kv × (key_length + value_length) × bytes_per_element
# KV_Per_Slot            = n_ctx × n_layers × KV_per_token_per_layer
#
# ------------------------------------------------------------------------------
# So Given these values, this is what you are looking at:
#
# Model   Context_Window   KV_Per_Slot      NSeqMax (Slots)
# 7B      8K               ~537 MB VRAM     2
# 70B     8K               ~1.3 GB VRAM     2
#
# No Caching:
# Total sequences allocated: 2 (no cache)
# 7B:  Slot Memory (2 × 537MB) ~1.07GB: Total VRAM: ~8.1GB
# 70B: Slot Memory (2 × 1.3GB) ~2.6GB : Total VRAM: ~72.6GB
#
# First Memory Caching (FMC):
# Total sequences allocated: 2 + 1 = 3 (cache)
# 7B:  Slot Memory (3 × 537MB) ~1.6GB: Total VRAM: ~8.6GB
# 70B: Slot Memory (3 × 1.3GB) ~3.9GB: Total VRAM: ~73.9GB
#
# Both SPC and FMC:
# Total sequences allocated: 2 + 2 = 4 (cache)
# 7B:  Slot Memory (4 × 537MB) ~2.15GB: Total VRAM: ~9.2GB
# 70B: Slot Memory (4 × 1.3GB) ~5.2GB:  Total VRAM: ~75.2GB
#
# ------------------------------------------------------------------------------
# Full Example With Real Model:
#
# Model                   : Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL
# Size                    : 36.0GB
# Context Window          : 131072 (128k)
# cache-type-k            : q8_0 (1 byte per element), f16 (2 bytes)
# cache-type-v            : q8_0 (1 byte per element), f16 (2 bytes)
# block_count             : 48  (n_layers)
# attention.head_count_kv : 4   (KV heads)
# attention.key_length    : 128	(K dimension per head)
# attention.value_length  : 128	(V dimension per head)
#
# KV_per_token_per_layer = head_count_kv  ×  (key_length + value_length)  ×  bytes_per_element
# 1024 bytes             =             4  ×  ( 128       +         128 )  ×  1
#
# KV_Per_Slot            =  n_ctx  ×  n_layers  ×  KV_per_token_per_layer
# ~6.4 GB                =  131072 ×  48        ×  1024
#
# No Caching:
# Total sequences allocated: 2 : (no cache)
# Slot Memory (2 × 6.4GB) ~12.8GB: Total VRAM: ~48.8GB
#
# First Memory Caching (FMC):
# Total sequences allocated: 3 : (2 + 1) (1 cache sequence)
# Slot Memory (3 × 6.4GB) ~19.2GB: Total VRAM: ~55.2GB
#
# Both SPC and FMC:
# Total sequences allocated: 4 : (2 + 2) (2 cache sequences)
# Slot Memory (4 × 6.4GB) ~25.6GB: Total VRAM: ~61.6GB

# ==============================================================================
# CONFIGURATION NOTES
#
# If you want to try using a model with an Agent, use these settings:
#   nseq-max: 1
#   first-message-cache: true
#
# If you want to use a Chat application like OpenWebUI use these settings:
#   system-prompt-cache: true
#
# Cline is working great with cerebras_qwen3-coder-reap-25b-a3b-q8_0
#
# Claude Code needs a model that handles tool calling well with a decent
# context window. The GPT models have not performed well. I have not
# found a model yet that does for Claude Code.

# ==============================================================================
# TEXT INFERENCE MODELS

# ------------------------------------------------------------------------------
# Coding Agents and configured for Cline or Claude Code

# This is a good model and has worked well Cline and is faster than the rest.
Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 1
  first-message-cache: true

# This is a good model and has worked well Cline.
GLM-4.7-Flash-UD-Q8_K_XL:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 1
  first-message-cache: true

# This is a decent model and has worked ok with Cline.
cerebras_qwen3-coder-reap-25b-a3b-q8_0:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 1
  first-message-cache: true

# This is a decent model and has worked ok with Cline.
Qwen3-Coder-30B-A3B-Instruct-Q8_0:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  nseq-max: 1
  first-message-cache: true

# ------------------------------------------------------------------------------
# Good Reasoning models, tooling support, don't use with Agents.

gpt-oss-120b-F16:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

gpt-oss-20b-Q8_0:
  context-window: 98304
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

# Great model but small context-window :(
Qwen3-8B-Q8_0:
  context-window: 40960
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

# ------------------------------------------------------------------------------
# Non-Reasoning models, tooling support, don't use with Agents.

# Model runs slow and haven't tested it too much.
GLM-4.7-Flash-Q8_0:
  context-window: 131072
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
  nseq-max: 2

# ==============================================================================
# VISION MODELS

# Vision model that is working great.
Qwen2.5-VL-3B-Instruct-Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 2048
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled

# ==============================================================================
# AUDIO MODELS

# Audio model that is working great.
Qwen2-Audio-7B.Q8_0:
  context-window: 8192
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled

# ==============================================================================
# EMBEDDING MODELS

# Embedding model that is working great.
embeddinggemma-300m-qat-Q8_0:
  context-window: 2048
  nbatch: 2048
  nubatch: 512
  cache-type-k: q8_0
  cache-type-v: q8_0
  flash-attention: enabled
